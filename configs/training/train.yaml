run_name: Yarn-Llama-2-7b-8k
output_dir: outputs
seed: 31337
num_steps: 400

accelerator:
  gradient_accumulation_steps: 8
  mixed_precision: bf16
  # fsdp_plugin: configured via accelerate config
  log_with: wandb
  project_dir: ${output_dir}/${run_name}

rope_scaling: 
  type: "yarn"
  factor: 2.0
  original_max_position_embeddings: 4096

model:
  pretrained_model_name_or_path: meta-llama/Llama-2-7b-hf
  attn_implementation: flash_attention_2

data:
  path: emozilla/pg_books-tokenized-bos-eos-chunked-65536
  map:
    num_proc: 32
  dataloader:
    batch_size: 1
    shuffle: true

optim:
  type: adamw
  params:
    lr: 2e-5
    eps: 1e-8
    weight_decay: 0.0
  betas: [0.9, 0.95]
  max_grad_norm: 1.0

scheduler:
  num_warmup_steps: 20
  num_training_steps: ${num_steps}